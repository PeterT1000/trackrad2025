{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "# !pip install gcapi\n",
    "import gcapi \n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper script to upload the private testing sets to Grand Challenge\n",
    "# The testing sets follows the same structure as the public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proide a api token with write access\n",
    "API_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_case_content_to_interfaces(case):\n",
    "    return {\n",
    "        \"frame-rate\": json.loads(Path(case[0]).read_text()),\n",
    "        \"magnetic-field-strength\": json.loads(Path(case[1]).read_text()),\n",
    "        \"scanned-region\": json.loads(Path(case[2]).read_text()),\n",
    "        \"mri-linac-series\": [Path(case[3])],\n",
    "        \"mri-linac-target\": [Path(case[4])],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GC archive to upload to\n",
    "\n",
    "# prelimary testing phase\n",
    "ARCHIVE_SLUG = \"trackrad2025-preliminary-testing-dataset\"\n",
    "DATASET_DIR = \"../dataset/preliminary\"\n",
    "\n",
    "# final testing phase\n",
    "ARCHIVE_SLUG = \"trackrad2025-final-testing-dataset\"\n",
    "DATASET_DIR = \"../dataset/testing\"\n",
    "\n",
    "# override for testing this script\n",
    "#DATASET_DIR = \"../dataset/example\"\n",
    "\n",
    "# safety switch to prevent accidental uploads\n",
    "# as uploads are NOT idempotent\n",
    "perform_upload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the expected cases to upload\n",
    "case_ids = sorted(os.listdir(DATASET_DIR))\n",
    "case_ids = [case_id for case_id in case_ids if os.path.isdir(f\"{DATASET_DIR}/{case_id}\")]\n",
    "\n",
    "COLLECTED_CASES_FILES = [\n",
    "    # for: frame-rate, magnetic-field-strength, scanned-region, mri-linac-series, mri-linac-target\n",
    "    [f\"{DATASET_DIR}/{case_id}/frame-rate.json\", \n",
    "     f\"{DATASET_DIR}/{case_id}/b-field-strength.json\", \n",
    "     f\"{DATASET_DIR}/{case_id}/scanned-region.json\", \n",
    "     f\"{DATASET_DIR}/{case_id}/images/{case_id}_frames.mha\", \n",
    "     f\"{DATASET_DIR}/{case_id}/targets/{case_id}_first_label.mha\"]\n",
    "    for case_id in case_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a sanity-check to see if we have all the files we expect\n",
    "for case_files in tqdm.tqdm(COLLECTED_CASES_FILES):\n",
    "    for file in case_files:\n",
    "        path = Path(file)\n",
    "        if not path.exists():\n",
    "            raise RuntimeError(f\"Could not find {path.absolute()}\")\n",
    "    # cheeck if the mapping is correct\n",
    "    content = map_case_content_to_interfaces(case_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_upload:\n",
    "    client = gcapi.Client(token=API_TOKEN)\n",
    "    archive = client.archives.detail(slug=ARCHIVE_SLUG)\n",
    "    archive_api_url = archive[\"api_url\"]\n",
    "\n",
    "    for case_files in tqdm.tqdm(COLLECTED_CASES_FILES):\n",
    "        content = map_case_content_to_interfaces(case_files)\n",
    "        archive_item = client.archive_items.create(archive=archive_api_url, values=[])\n",
    "        client.update_archive_item(archive_item_pk=archive_item[\"pk\"], values=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare ground truth archive .tar.gz using python\n",
    "# only contains\n",
    "# {case_id}/targets/{case_id}_labels.mha\n",
    "\n",
    "import tarfile\n",
    "\n",
    "filename = os.path.join(os.path.dirname(DATASET_DIR), os.path.basename(DATASET_DIR) + \"_gt.tar.gz\")\n",
    "\n",
    "with tarfile.open(filename, \"w:gz\") as tar:\n",
    "    for case_id in case_ids:\n",
    "        target = f\"{DATASET_DIR}/{case_id}/targets/{case_id}_labels.mha\"\n",
    "        if not os.path.exists(target):\n",
    "            raise RuntimeError(f\"Could not find {target}\")\n",
    "        tar.add(target, arcname=f\"{case_id}/targets/{case_id}_labels.mha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the tar.gz is correct and matches the full dataset\n",
    "with tarfile.open(filename, \"r:gz\") as tar:\n",
    "    for member in tar.getmembers():\n",
    "        print(member.name, member.size)\n",
    "        case_id = member.name.split(\"/\")[0]\n",
    "        !ls -l $DATASET_DIR/{case_id}/targets/{case_id}_labels.mha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
